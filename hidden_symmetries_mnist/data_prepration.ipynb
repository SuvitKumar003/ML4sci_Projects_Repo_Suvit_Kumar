{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms.functional import rotate\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This code is a PyTorch dataset class that loads the MNIST dataset and applies rotations to the images.\n",
    "class RotatedMNIST(Dataset):\n",
    "    def __init__(self, root, train=True, digits=(1, 2)):\n",
    "        self.mnist = MNIST(root=root, train=train, download=True,\n",
    "                          transform=ToTensor())  # Convert to tensor upfront\n",
    "        self.indices = [i for i, (_, label) in enumerate(self.mnist) \n",
    "                      if label in digits]\n",
    "        self.angles = list(range(0, 360, 30))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices) * len(self.angles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_idx = self.indices[idx // len(self.angles)]\n",
    "        angle = self.angles[idx % len(self.angles)]\n",
    "        img, label = self.mnist[img_idx]\n",
    "        \n",
    "        # Rotate tensor directly (no PIL conversion)\n",
    "        img_rotated = rotate(img, angle)\n",
    "        return img_rotated, label, torch.tensor(angle/360.0, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts to [0, 1] range automatically\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting VAE Training...\n",
      "Epoch 1, Loss: 154.2721\n",
      "Epoch 2, Loss: 112.8621\n",
      "Epoch 3, Loss: 107.0420\n",
      "Epoch 4, Loss: 104.5923\n",
      "Epoch 5, Loss: 103.0775\n",
      "Epoch 6, Loss: 101.9875\n",
      "Epoch 7, Loss: 101.1702\n",
      "Epoch 8, Loss: 100.6196\n",
      "Epoch 9, Loss: 100.1122\n",
      "Epoch 10, Loss: 99.7227\n",
      "Epoch 11, Loss: 99.4339\n",
      "Epoch 12, Loss: 99.0939\n",
      "Epoch 13, Loss: 98.8460\n",
      "Epoch 14, Loss: 98.6016\n",
      "Epoch 15, Loss: 98.3999\n",
      "Epoch 16, Loss: 98.1911\n",
      "Epoch 17, Loss: 98.0220\n",
      "Epoch 18, Loss: 97.8141\n",
      "Epoch 19, Loss: 97.7028\n",
      "Epoch 20, Loss: 97.5630\n"
     ]
    }
   ],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=32):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*7*7, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.fc_mu = nn.Linear(256, latent_dim)\n",
    "        self.fc_var = nn.Linear(256, latent_dim)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64*7*7),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (64, 7, 7)),\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.fc_mu(h), self.fc_var(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VAE(latent_dim=32).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "print(\"Starting VAE Training...\")\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)  \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = vae_loss(recon_batch, data, mu, logvar)\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f'Epoch {epoch+1}, Loss: {train_loss/len(train_loader.dataset):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"vae_mnist.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Flatten(start_dim=1, end_dim=-1)\n",
       "    (5): Linear(in_features=3136, out_features=256, bias=True)\n",
       "    (6): ReLU()\n",
       "  )\n",
       "  (fc_mu): Linear(in_features=256, out_features=32, bias=True)\n",
       "  (fc_var): Linear(in_features=256, out_features=32, bias=True)\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=3136, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Unflatten(dim=1, unflattened_size=(64, 7, 7))\n",
       "    (5): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (6): ReLU()\n",
       "    (7): ConvTranspose2d(32, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (8): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"vae_mnist.pth\"))\n",
    "model.train()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Loss: 97.4159\n",
      "Epoch 22, Loss: 97.3002\n",
      "Epoch 23, Loss: 97.1484\n",
      "Epoch 24, Loss: 97.0593\n",
      "Epoch 25, Loss: 96.8989\n",
      "Epoch 26, Loss: 96.7997\n",
      "Epoch 27, Loss: 96.7098\n",
      "Epoch 28, Loss: 96.6413\n",
      "Epoch 29, Loss: 96.5300\n",
      "Epoch 30, Loss: 96.4831\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(21, 31):  \n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = vae_loss(recon_batch, data, mu, logvar)\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch}, Loss: {train_loss/len(train_loader.dataset):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m angles \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data, _, angle \u001b[38;5;129;01min\u001b[39;00m train_loader:  \u001b[38;5;66;03m# Assuming your dataset returns (image, label, angle)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      8\u001b[0m         mu, logvar \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(data)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "# Extract latent representations for training the MLP\n",
    "latent_vectors = []\n",
    "angles = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, _, angle in train_loader:  # Assuming your dataset returns (image, label, angle)\n",
    "        data = data.to(device)\n",
    "        mu, logvar = model.encode(data)\n",
    "        z = model.reparameterize(mu, logvar)\n",
    "        \n",
    "        latent_vectors.append(z.cpu())\n",
    "        angles.append(angle.cpu())\n",
    "\n",
    "latent_vectors = torch.cat(latent_vectors, dim=0)\n",
    "angles = torch.cat(angles, dim=0)\n",
    "\n",
    "# Define the MLP for learning transformations in latent space\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, latent_dim=32):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, latent_dim)  # Output should be the same size as latent_dim\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "mlp = MLP(latent_dim=32).to(device)\n",
    "mlp_optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train the MLP\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(len(latent_vectors)):\n",
    "        z = latent_vectors[i].to(device)\n",
    "        angle = angles[i].to(device)\n",
    "\n",
    "        mlp_optimizer.zero_grad()\n",
    "        \n",
    "        z_rotated = mlp(z)  # Predict transformed latent vector\n",
    "        loss = criterion(z_rotated, z)  # Compare with original latent vector (self-supervised)\n",
    "        \n",
    "        loss.backward()\n",
    "        mlp_optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(latent_vectors):.4f}\")\n",
    "\n",
    "# Save the trained MLP model\n",
    "torch.save(mlp.state_dict(), \"mlp_symmetry.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=32):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.encoder_fc = nn.Linear(12544, 256)  \n",
    "        self.fc_mu = nn.Linear(256, latent_dim)\n",
    "        self.fc_var = nn.Linear(256, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_fc = nn.Linear(latent_dim, 256)\n",
    "        self.decoder_upsample = nn.Linear(256, 12544)  \n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = torch.flatten(x, start_dim=1)  \n",
    "        x = self.encoder_fc(x)\n",
    "        return self.fc_mu(x), self.fc_var(x)\n",
    "\n",
    "    def decode(self, z):\n",
    "        x = self.decoder_fc(z)\n",
    "        x = self.decoder_upsample(x)\n",
    "        x = x.view(-1, 64, 14, 14)  \n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        recon_x = self.decode(z)\n",
    "        return recon_x, mu, log_var  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae = VAE()\n",
    "checkpoint = torch.load(\"vae_mnist.pth\", map_location=device)\n",
    "model_dict = vae.state_dict()\n",
    "\n",
    "filtered_checkpoint = {k: v for k, v in checkpoint.items() if k in model_dict and v.shape == model_dict[k].shape}\n",
    "\n",
    "model_dict.update(filtered_checkpoint)\n",
    "vae.load_state_dict(model_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    recon_x, _, _ = vae(sample_input)  \n",
    "print(recon_x.shape)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with torch.no_grad():\n",
    "    recon_x, _, _ = vae(sample_input)\n",
    "\n",
    "input_img = sample_input.cpu().squeeze().numpy()\n",
    "recon_img = recon_x.cpu().squeeze().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(input_img, cmap='gray')\n",
    "ax[0].set_title(\"Original Image\")\n",
    "ax[0].axis(\"off\")\n",
    "\n",
    "ax[1].imshow(recon_img, cmap='gray')\n",
    "ax[1].set_title(\"Reconstructed Image\")\n",
    "ax[1].axis(\"off\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def get_latent_vectors(vae, dataloader, device=\"cuda\"):\n",
    "    vae.eval()\n",
    "    latent_vectors = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, _ in dataloader:\n",
    "            imgs = imgs.to(device)\n",
    "            mean, logvar = vae.encoder(imgs)\n",
    "            z = vae.reparameterize(mean, logvar)\n",
    "            latent_vectors.append(z.cpu().numpy())\n",
    "    return np.concatenate(latent_vectors, axis=0)\n",
    "\n",
    "def visualize_pca(latent_vectors):\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced = pca.fit_transform(latent_vectors)\n",
    "    plt.scatter(reduced[:, 0], reduced[:, 1], alpha=0.5)\n",
    "    plt.title(\"PCA of Latent Space\")\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.show()\n",
    "\n",
    "def cluster_latent_space(latent_vectors, n_clusters=4):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(latent_vectors)\n",
    "    return labels\n",
    "\n",
    "# Usage:\n",
    "# latent_vectors = get_latent_vectors(trained_vae, test_dataloader)\n",
    "# visualize_pca(latent_vectors)\n",
    "# labels = cluster_latent_space(latent_vectors)\n",
    "# print(\"Cluster Labels:\", labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['encoder.0.weight', 'encoder.0.bias', 'encoder.2.weight', 'encoder.2.bias', 'encoder.5.weight', 'encoder.5.bias', 'fc_mu.weight', 'fc_mu.bias', 'fc_var.weight', 'fc_var.bias', 'decoder.0.weight', 'decoder.0.bias', 'decoder.2.weight', 'decoder.2.bias', 'decoder.5.weight', 'decoder.5.bias', 'decoder.7.weight', 'decoder.7.bias'])\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(\"vae_mnist.pth\")\n",
    "print(checkpoint.keys())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (encoder_fc): Linear(in_features=12544, out_features=256, bias=True)\n",
      "  (fc_mu): Linear(in_features=256, out_features=32, bias=True)\n",
      "  (fc_var): Linear(in_features=256, out_features=32, bias=True)\n",
      "  (decoder_fc): Linear(in_features=32, out_features=256, bias=True)\n",
      "  (decoder_upsample): Linear(in_features=256, out_features=12544, bias=True)\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): ConvTranspose2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fixed model saved as 'vae_mnist_fixed.pth'. Now, load this into your VAE class.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "old_state_dict = torch.load(\"vae_mnist.pth\")\n",
    "\n",
    "new_state_dict = {}\n",
    "\n",
    "for key in old_state_dict.keys():\n",
    "    new_key = key  \n",
    "\n",
    "    if \"encoder.5\" in key:  # Fix missing layer issue\n",
    "        new_key = key.replace(\"encoder.5\", \"encoder.3\")  \n",
    "\n",
    "    # Fix decoder layer names\n",
    "    if \"decoder.5\" in key:\n",
    "        new_key = key.replace(\"decoder.5\", \"decoder.3\")\n",
    "    if \"decoder.7\" in key:\n",
    "        new_key = key.replace(\"decoder.7\", \"decoder.4\")  # Adjust numbering\n",
    "\n",
    "    # Fix decoder_fc and decoder_upsample (remove if necessary)\n",
    "    if \"decoder_fc\" in key or \"decoder_upsample\" in key:\n",
    "        continue  # Skip these keys, as they don't exist in the saved model\n",
    "\n",
    "    # Add the fixed key-value pair to the new dictionary\n",
    "    new_state_dict[new_key] = old_state_dict[key]\n",
    "\n",
    "# Save the corrected state dictionary\n",
    "torch.save(new_state_dict, \"vae_mnist_fixed.pth\")\n",
    "\n",
    "print(\"✅ Fixed model saved as 'vae_mnist_fixed.pth'. Now, load this into your VAE class.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for VAE:\n\tMissing key(s) in state_dict: \"encoder_fc.weight\", \"encoder_fc.bias\", \"decoder_fc.weight\", \"decoder_fc.bias\", \"decoder_upsample.weight\", \"decoder_upsample.bias\". \n\tUnexpected key(s) in state_dict: \"encoder.3.weight\", \"encoder.3.bias\", \"decoder.4.weight\", \"decoder.4.bias\", \"decoder.3.weight\", \"decoder.3.bias\". \n\tsize mismatch for decoder.0.weight: copying a param with shape torch.Size([256, 32]) from checkpoint, the shape in current model is torch.Size([64, 32, 3, 3]).\n\tsize mismatch for decoder.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for decoder.2.weight: copying a param with shape torch.Size([3136, 256]) from checkpoint, the shape in current model is torch.Size([32, 1, 3, 3]).\n\tsize mismatch for decoder.2.bias: copying a param with shape torch.Size([3136]) from checkpoint, the shape in current model is torch.Size([1]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m trained_vae \u001b[38;5;241m=\u001b[39m VAE(latent_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)  \u001b[38;5;66;03m# Ensure latent_dim matches\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrained_vae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvae_mnist_fixed.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# strict=True should now work\u001b[39;00m\n\u001b[0;32m      3\u001b[0m trained_vae\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Move to GPU if available\u001b[39;00m\n\u001b[0;32m      4\u001b[0m trained_vae\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set to evaluation mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2581\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2573\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2574\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2575\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2576\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2577\u001b[0m             ),\n\u001b[0;32m   2578\u001b[0m         )\n\u001b[0;32m   2580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2581\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2583\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2584\u001b[0m         )\n\u001b[0;32m   2585\u001b[0m     )\n\u001b[0;32m   2586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for VAE:\n\tMissing key(s) in state_dict: \"encoder_fc.weight\", \"encoder_fc.bias\", \"decoder_fc.weight\", \"decoder_fc.bias\", \"decoder_upsample.weight\", \"decoder_upsample.bias\". \n\tUnexpected key(s) in state_dict: \"encoder.3.weight\", \"encoder.3.bias\", \"decoder.4.weight\", \"decoder.4.bias\", \"decoder.3.weight\", \"decoder.3.bias\". \n\tsize mismatch for decoder.0.weight: copying a param with shape torch.Size([256, 32]) from checkpoint, the shape in current model is torch.Size([64, 32, 3, 3]).\n\tsize mismatch for decoder.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([32]).\n\tsize mismatch for decoder.2.weight: copying a param with shape torch.Size([3136, 256]) from checkpoint, the shape in current model is torch.Size([32, 1, 3, 3]).\n\tsize mismatch for decoder.2.bias: copying a param with shape torch.Size([3136]) from checkpoint, the shape in current model is torch.Size([1])."
     ]
    }
   ],
   "source": [
    "trained_vae = VAE(latent_dim=32)  # Ensure latent_dim matches\n",
    "trained_vae.load_state_dict(torch.load(\"vae_mnist_fixed.pth\"), strict=True)  # strict=True should now work\n",
    "trained_vae.to(\"cuda\")  # Move to GPU if available\n",
    "trained_vae.eval()  # Set to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trained_vae' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 57\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mean_distance\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Steps to execute:\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# 1. Extract latent vectors for original and rotated images\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m latent_vectors, rotated_latent_vectors, img_labels \u001b[38;5;241m=\u001b[39m get_latent_vectors(\u001b[43mtrained_vae\u001b[49m, test_dataloader)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# 2. Visualize latent space\u001b[39;00m\n\u001b[0;32m     60\u001b[0m visualize_pca(latent_vectors, img_labels, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Latent Space\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trained_vae' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Assuming we have a trained VAE encoder\n",
    "# Function to extract latent representations\n",
    "def get_latent_vectors(vae, dataloader, device=\"cuda\"):\n",
    "    vae.eval()\n",
    "    latent_vectors = []\n",
    "    rotated_latent_vectors = []\n",
    "    labels_list = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in dataloader:\n",
    "            imgs = imgs.to(device)\n",
    "            mean, logvar = vae.encoder(imgs)\n",
    "            z = vae.reparameterize(mean, logvar)\n",
    "            latent_vectors.append(z.cpu().numpy())\n",
    "            labels_list.append(labels.numpy())\n",
    "\n",
    "            # Rotate images by 30 degrees and get new latent vectors\n",
    "            rotated_imgs = torch.rot90(imgs, k=1, dims=[2, 3])  # 90-degree rotation as an example\n",
    "            mean_r, logvar_r = vae.encoder(rotated_imgs)\n",
    "            z_rotated = vae.reparameterize(mean_r, logvar_r)\n",
    "            rotated_latent_vectors.append(z_rotated.cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(latent_vectors, axis=0), np.concatenate(rotated_latent_vectors, axis=0), np.concatenate(labels_list, axis=0)\n",
    "\n",
    "# Perform PCA to visualize the latent space\n",
    "def visualize_pca(latent_vectors, labels, title=\"PCA of Latent Space\"):\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced = pca.fit_transform(latent_vectors)\n",
    "    plt.scatter(reduced[:, 0], reduced[:, 1], c=labels, cmap='viridis', alpha=0.5)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "# Perform clustering to find symmetry patterns\n",
    "def cluster_latent_space(latent_vectors, n_clusters=4):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(latent_vectors)\n",
    "    return labels, kmeans.cluster_centers_\n",
    "\n",
    "# Validate discovered symmetries by comparing rotated cluster assignments\n",
    "def validate_symmetry(original_centers, rotated_centers):\n",
    "    distances = cdist(original_centers, rotated_centers, metric='euclidean')\n",
    "    mean_distance = np.mean(np.min(distances, axis=1))\n",
    "    print(f\"Mean distance between original and rotated clusters: {mean_distance}\")\n",
    "    return mean_distance\n",
    "\n",
    "# Steps to execute:\n",
    "# 1. Extract latent vectors for original and rotated images\n",
    "latent_vectors, rotated_latent_vectors, img_labels = get_latent_vectors(trained_vae, test_dataloader)\n",
    "\n",
    "# 2. Visualize latent space\n",
    "visualize_pca(latent_vectors, img_labels, title=\"Original Latent Space\")\n",
    "visualize_pca(rotated_latent_vectors, img_labels, title=\"Rotated Latent Space\")\n",
    "\n",
    "# 3. Perform clustering\n",
    "cluster_labels, cluster_centers = cluster_latent_space(latent_vectors)\n",
    "rotated_cluster_labels, rotated_cluster_centers = cluster_latent_space(rotated_latent_vectors)\n",
    "\n",
    "# 4. Validate symmetry\n",
    "mean_dist = validate_symmetry(cluster_centers, rotated_cluster_centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bekar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
