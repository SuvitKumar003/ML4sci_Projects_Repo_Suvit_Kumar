{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms.functional import rotate\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor, functional as F\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class RotatedMNIST(Dataset):\n",
    "    def __init__(self, root=\"./data\", train=True, digits=(1, 2), angles=None):\n",
    "        self.mnist = MNIST(root=root, train=train, download=True, transform=ToTensor())\n",
    "        self.indices = [i for i, (_, label) in enumerate(self.mnist) if label in digits]\n",
    "        self.angles = angles if angles else list(range(0, 360, 30))  # Rotations in steps of 30 degrees\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices) * len(self.angles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_idx = self.indices[idx // len(self.angles)]\n",
    "        angle = self.angles[idx % len(self.angles)]\n",
    "        img, label = self.mnist[img_idx]\n",
    "\n",
    "        img_rotated = F.rotate(img, angle)\n",
    "        return img, img_rotated, torch.tensor(angle / 360.0)  # Normalize angle\n",
    "\n",
    "# Define dataset and DataLoader\n",
    "train_dataset = RotatedMNIST(root=\"./data\", train=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts to [0, 1] range automatically\n",
    "])\n",
    "\n",
    "# Load Train and Test Datasets\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)  # Fix: Define test_dataset\n",
    "\n",
    "# Define Data Loaders\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)  # Fix: Now test_dataset is defined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case running the code for the first time then train the whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting VAE Training...\n",
      "Epoch 1, Loss: 183.2755\n",
      "Epoch 2, Loss: 123.7812\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 66\u001b[0m\n\u001b[0;32m     63\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)  \n\u001b[0;32m     64\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 66\u001b[0m recon_batch, mu, logvar \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m loss \u001b[38;5;241m=\u001b[39m vae_loss(recon_batch, data, mu, logvar)\n\u001b[0;32m     69\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[10], line 46\u001b[0m, in \u001b[0;36mVAE.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     44\u001b[0m mu, logvar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode(x)\n\u001b[0;32m     45\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreparameterize(mu, logvar)\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m, mu, logvar\n",
      "Cell \u001b[1;32mIn[10], line 41\u001b[0m, in \u001b[0;36mVAE.decode\u001b[1;34m(self, z)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, z):\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:1162\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[1;34m(self, input, output_size)\u001b[0m\n\u001b[0;32m   1151\u001b[0m num_spatial_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m   1152\u001b[0m output_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_padding(\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1154\u001b[0m     output_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1159\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1160\u001b[0m )\n\u001b[1;32m-> 1162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_transpose2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1165\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1166\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1167\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1168\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1169\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1170\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1171\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=32):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*7*7, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.fc_mu = nn.Linear(256, latent_dim)\n",
    "        self.fc_var = nn.Linear(256, latent_dim)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64*7*7),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (64, 7, 7)),\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.fc_mu(h), self.fc_var(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VAE(latent_dim=32).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "print(\"Starting VAE Training...\")\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)  \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = vae_loss(recon_batch, data, mu, logvar)\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f'Epoch {epoch+1}, Loss: {train_loss/len(train_loader.dataset):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the model is already saved then use this code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained VAE model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the VAE model\n",
    "vae = VAE().to(device)\n",
    "\n",
    "# Load the pre-trained weights\n",
    "checkpoint = torch.load(\"vae_mnist.pth\", map_location=device)\n",
    "vae.load_state_dict(checkpoint)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "vae.eval()\n",
    "\n",
    "print(\"Pre-trained VAE model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"vae_mnist.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Flatten(start_dim=1, end_dim=-1)\n",
       "    (5): Linear(in_features=3136, out_features=256, bias=True)\n",
       "    (6): ReLU()\n",
       "  )\n",
       "  (fc_mu): Linear(in_features=256, out_features=32, bias=True)\n",
       "  (fc_var): Linear(in_features=256, out_features=32, bias=True)\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=3136, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Unflatten(dim=1, unflattened_size=(64, 7, 7))\n",
       "    (5): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (6): ReLU()\n",
       "    (7): ConvTranspose2d(32, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (8): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"vae_mnist.pth\"))\n",
    "model.train()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Loss: 97.4159\n",
      "Epoch 22, Loss: 97.3002\n",
      "Epoch 23, Loss: 97.1484\n",
      "Epoch 24, Loss: 97.0593\n",
      "Epoch 25, Loss: 96.8989\n",
      "Epoch 26, Loss: 96.7997\n",
      "Epoch 27, Loss: 96.7098\n",
      "Epoch 28, Loss: 96.6413\n",
      "Epoch 29, Loss: 96.5300\n",
      "Epoch 30, Loss: 96.4831\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(21, 31):  \n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = vae_loss(recon_batch, data, mu, logvar)\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch}, Loss: {train_loss/len(train_loader.dataset):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.4625\n",
      "Epoch 2/20, Loss: 0.4607\n",
      "Epoch 3/20, Loss: 0.4605\n",
      "Epoch 4/20, Loss: 0.4604\n",
      "Epoch 5/20, Loss: 0.4603\n",
      "Epoch 6/20, Loss: 0.4603\n",
      "Epoch 7/20, Loss: 0.4602\n",
      "Epoch 8/20, Loss: 0.4603\n",
      "Epoch 9/20, Loss: 0.4603\n",
      "Epoch 10/20, Loss: 0.4602\n",
      "Epoch 11/20, Loss: 0.4602\n",
      "Epoch 12/20, Loss: 0.4602\n",
      "Epoch 13/20, Loss: 0.4602\n",
      "Epoch 14/20, Loss: 0.4602\n",
      "Epoch 15/20, Loss: 0.4602\n",
      "Epoch 16/20, Loss: 0.4602\n",
      "Epoch 17/20, Loss: 0.4602\n",
      "Epoch 18/20, Loss: 0.4602\n",
      "Epoch 19/20, Loss: 0.4602\n",
      "Epoch 20/20, Loss: 0.4602\n"
     ]
    }
   ],
   "source": [
    "# Extract latent representations for training the MLP\n",
    "latent_vectors = []\n",
    "rotated_latent_vectors = []\n",
    "angles = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img, img_rotated, angle in train_loader:\n",
    "        img, img_rotated = img.to(device), img_rotated.to(device)\n",
    "\n",
    "        mu_original, _ = vae.encode(img)  # Encode original image\n",
    "        mu_rotated, _ = vae.encode(img_rotated)  # Encode rotated image\n",
    "\n",
    "        latent_vectors.append(mu_original.cpu())\n",
    "        rotated_latent_vectors.append(mu_rotated.cpu())\n",
    "        angles.append(angle.cpu())\n",
    "\n",
    "latent_vectors = torch.cat(latent_vectors, dim=0)\n",
    "rotated_latent_vectors = torch.cat(rotated_latent_vectors, dim=0)\n",
    "angles = torch.cat(angles, dim=0)\n",
    "\n",
    "\n",
    "# Define the MLP for learning transformations in latent space\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, latent_dim=32):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, latent_dim)  # Output should match latent dimension\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Initialize MLP\n",
    "mlp = MLP(latent_dim=32).to(device)\n",
    "mlp_optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Train MLP\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(len(latent_vectors)):\n",
    "        z = latent_vectors[i].to(device)\n",
    "        z_rotated = rotated_latent_vectors[i].to(device)\n",
    "\n",
    "        mlp_optimizer.zero_grad()\n",
    "        \n",
    "        z_predicted = mlp(z)  # Predict rotated latent space\n",
    "        loss = criterion(z_predicted, z_rotated)  # Compare with actual rotated latent vector\n",
    "        \n",
    "        loss.backward()\n",
    "        mlp_optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(latent_vectors):.4f}\")\n",
    "\n",
    "# Save trained MLP model\n",
    "torch.save(mlp.state_dict(), \"mlp_symmetry.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=32):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.encoder_fc = nn.Linear(12544, 256)  \n",
    "        self.fc_mu = nn.Linear(256, latent_dim)\n",
    "        self.fc_var = nn.Linear(256, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_fc = nn.Linear(latent_dim, 256)\n",
    "        self.decoder_upsample = nn.Linear(256, 12544)  \n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = torch.flatten(x, start_dim=1)  \n",
    "        x = self.encoder_fc(x)\n",
    "        return self.fc_mu(x), self.fc_var(x)\n",
    "\n",
    "    def decode(self, z):\n",
    "        x = self.decoder_fc(z)\n",
    "        x = self.decoder_upsample(x)\n",
    "        x = x.view(-1, 64, 14, 14)  \n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        recon_x = self.decode(z)\n",
    "        return recon_x, mu, log_var  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae = VAE()\n",
    "checkpoint = torch.load(\"vae_mnist.pth\", map_location=device)\n",
    "model_dict = vae.state_dict()\n",
    "\n",
    "filtered_checkpoint = {k: v for k, v in checkpoint.items() if k in model_dict and v.shape == model_dict[k].shape}\n",
    "\n",
    "model_dict.update(filtered_checkpoint)\n",
    "vae.load_state_dict(model_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    recon_x, _, _ = vae(sample_input)  \n",
    "print(recon_x.shape)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with torch.no_grad():\n",
    "    recon_x, _, _ = vae(sample_input)\n",
    "\n",
    "input_img = sample_input.cpu().squeeze().numpy()\n",
    "recon_img = recon_x.cpu().squeeze().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(input_img, cmap='gray')\n",
    "ax[0].set_title(\"Original Image\")\n",
    "ax[0].axis(\"off\")\n",
    "\n",
    "ax[1].imshow(recon_img, cmap='gray')\n",
    "ax[1].set_title(\"Reconstructed Image\")\n",
    "ax[1].axis(\"off\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\anaconda3\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'VAE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Load trained VAE model\u001b[39;00m\n\u001b[0;32m     10\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m vae \u001b[38;5;241m=\u001b[39m \u001b[43mVAE\u001b[49m()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     12\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvae_mnist.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     13\u001b[0m vae\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'VAE' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms.functional import rotate\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Load trained VAE model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vae = VAE().to(device)\n",
    "checkpoint = torch.load(\"vae_mnist.pth\", map_location=device)\n",
    "vae.load_state_dict(checkpoint)\n",
    "vae.eval()\n",
    "\n",
    "# Define MLP model for transformation learning\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, latent_dim=32):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Initialize MLP\n",
    "mlp = MLP(latent_dim=32).to(device)\n",
    "optimizer = optim.Adam(mlp.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Prepare Rotated MNIST Dataset (digits 1 and 2 only)\n",
    "class RotatedMNIST(Dataset):\n",
    "    def __init__(self, root, train=True, digits=(1, 2)):\n",
    "        self.mnist = MNIST(root=root, train=train, download=True, transform=ToTensor())\n",
    "        self.indices = [i for i, (_, label) in enumerate(self.mnist) if label in digits]\n",
    "        self.angles = list(range(0, 360, 30))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices) * len(self.angles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_idx = self.indices[idx // len(self.angles)]\n",
    "        angle = self.angles[idx % len(self.angles)]\n",
    "        img, label = self.mnist[img_idx]\n",
    "\n",
    "        img_rotated = rotate(img, angle)\n",
    "        return img, img_rotated, angle  # Return original, rotated, and angle\n",
    "\n",
    "# DataLoader\n",
    "train_dataset = RotatedMNIST(root=\"./data\", train=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Train MLP\n",
    "num_epochs = 10  # Adjust if needed\n",
    "vae.eval()\n",
    "mlp.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for img, img_rotated, angle in train_loader:\n",
    "        img, img_rotated = img.to(device), img_rotated.to(device)\n",
    "\n",
    "        # Encode images to latent space\n",
    "        with torch.no_grad():\n",
    "            _, mu_original, _ = vae(img)\n",
    "            _, mu_rotated, _ = vae(img_rotated)\n",
    "\n",
    "        # Train MLP to learn the transformation in latent space\n",
    "        optimizer.zero_grad()\n",
    "        predicted_rotated = mlp(mu_original)\n",
    "        loss = criterion(predicted_rotated, mu_rotated)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Save the trained MLP model\n",
    "torch.save(mlp.state_dict(), \"mlp_rotation_mapper.pth\")\n",
    "\n",
    "# Test the trained MLP (Example)\n",
    "with torch.no_grad():\n",
    "    img, img_rotated, _ = train_dataset[0]\n",
    "    img = img.unsqueeze(0).to(device)  # Add batch dimension\n",
    "    _, mu_original, _ = vae(img)\n",
    "    predicted_latent = mlp(mu_original)\n",
    "    reconstructed_rotated = vae.decode(predicted_latent)\n",
    "\n",
    "    print(\"Original image and predicted rotated image processed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
